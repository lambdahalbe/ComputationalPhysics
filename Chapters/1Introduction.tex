\chapter{Introduction}
\pagenumbering{arabic}

Matter consists of a very large number of interacting particles. In general, we cannot solve their equations of motion exactly $\Rightarrow$ We need to develop numerical methods.\\
If we set out to predict the properties of macroscopic objects from the quantum mechanical description of all their constituents, even numerical methods fail. We cannot handle the time-dependent \textsc{Schr√∂dinger} equation of $10^{23}$ particles, no matter how much of the computer power available today in the world we have access to.\\
Thus, in addition to numerics, we will need statistical mechanics.\\
In this course we will discuss methods to solve statistical mechanics problems on the computer. We begin the course with a brief reminder of the main concepts that lead to the development of statistical mechanics.\\

In the 19\textsuperscript{th} century, the theory of \emph{Thermodynamics} was developed to predict the efficiency of thermal machines, such as the steam engine. Thermodynamics is a macroscopic theory. It deals with large objects and, in particular, time-independent properties (i.e. ``equilibrium''). The central notion of Thermodynamics is the fundamental equation, which relates entropy $S$ to internal energy $U$, the particle Number $N$ and the Volume $V$ of an isolated system.

\begin{align}
S &= f(U, V, N)
\end{align}

As $S$ is monotonic in $U$ (this is one of the axioms of Thermodynamics), we can invert $f$ to obtain $U = g(S, V, N)$. The intensive quantities pressure $p$, temperature $T$ and chemical potential $\mu$ are then defined as derivatives of $g$ with respect to $V$, $S$ and $N$.\\
If one knows the fundamental equations for a specific system, one knows its entire equilibrium thermodynamic behaviour, i.e. its reaction to changes in $U$, $V$, $N$ or the conjugate intensive quantities. The latter can be fixed by means of baths ( ``reservoirs''). To transform the fundamental equation to account for the fixed $T$, $p$ or $\mu$ without loss of information, we simply use a \textsc{Legendre} transformation ($\rightarrow$ therm. potentials).\\
For the entropy of an isolated system there holds an extremization principle, which is carried over to the other thermodynamic potentials (e.g. \cref{pic:free_energy}).\\

\begin{figure}
\centering
\begin{tikzpicture}[scale=.8]
\draw (0,0) node[below right] {$T$} rectangle (3.5,-1.5);
\draw (1, -.3) node[below right] {$N$, $V$, $U$} rectangle (3,-1.2);
\end{tikzpicture}
\caption{A system in contact with a heat bath (and fixed particle number and volume) takes the thermodynamic state which minimizes \emph{free energy}:
$F(N, V, T) = U - TS$}
\label{pic:free_energy}
\end{figure}
\newpage

We have intuitive notions for $N$ and $V$. And from (quantum) mechanics, we know how to compute energy. But in the 19\textsuperscript{th} century, the microscopic origin of entropy was not clear.\\

Through the works of \textsc{Boltzmann}, \textsc{Gibbs} and others, its relation to the distribution of microstates was discovered:

\begin{figure}[!h]
\centering
\begin{tikzpicture}[]
\draw (0, 0) node [below right] {$T$\quad heat bath} rectangle (4, -3);
\draw (1, -.5) node [below right] {$V$} rectangle (3.5, -2.5);
\node at (1.5, -2.3) {$H(\vec{\pi})$};
\filldraw (1.5, -1.5) circle (.03) (2, -1.9) circle (.03) (2.3,-1) circle (.03) (3.3, -2.2) circle (.03) -- (5,-1) node [right] {$N$ particles};
\node[right] at (5, -1.5) {with Hamiltonian $H(\vec{\pi})$, $\pi\in\Gamma$ ``phase space''};
\filldraw (5, -1) -- (2.8, -1.3) circle (.03);
\end{tikzpicture}
\caption{Schema: System in heat bath}
\end{figure}

probability to find a state $\vec{\pi}$ in $\Gamma$: 
\begin{align}
\rho(\vec{\pi}) &= \frac{e^{-\beta H(\vec{\pi})}}{\int e^{-\beta H(\vec{\pi}^\prime)}\ \dd\Gamma^\prime}\\
Z &:= \int e^{-\beta H(\vec{\pi})}\ \dd\Gamma \qquad\text{ ``partition function''}\\
F(N, V, T) &= -\underbrace{k_B}_{\parbox{0cm}{\text{\textsc{\hspace*{-1.5cm}Boltzmann}'s constant}}}T\ln(Z)
\end{align}

If we measure an observable $A(\vec{\pi})$ in equilibrium, we will on average find the value:
\begin{align}
	\langle A\rangle_{N, V, T} &= \frac{\int A(\vec{\pi})e^{-\beta H(\vec{\pi})}\ \dd\Gamma}{Z}
\end{align}

The first part of this course will deal with methods to estimate such values.\\
In microscopic systems, $H$ can be separated in a kinetic and a potential part:

\begin{align}
	H(\vec{\pi}) &= V(\vec{r}^N) + K(\vec{r}^N) \quad\text{with}\quad \vec{r}^N = \{\vec{r}_1,\dots,\vec{r}_N\}
\end{align}

(This is not necessarily true in macroscopic systems, where friction might vary spatially, i.e. velocity and position are coupled.)\\

Often we are interested in observables that depend only on position (e.g. the structure of a crystal, the magnetization of a ferromagnet).\\
Then

\begin{align}
	\langle A \rangle_{N, V, T} &= \frac{\int\dd\vec{r}^N\dd\vec{p}^N A\left(\vec{r}^N\right) e^{-\beta\left(V\left(\vec{r}^N\right) + K\left(\vec{p}^N\right)\right)}}{\int\dd\vec{r}^N\dd\vec{p}^N e^{-\beta\left(V\left(\vec{r}^N\right) + K\left(\vec{p}^N\right)\right)}}\\
	&= \frac{\int\dd\vec{r}^N A\left(\vec{r}^N\right) e^{-\beta V\left(\vec{r}^N\right)}}{\underbrace{\textstyle\int\dd\vec{r}^N e^{-\beta V\left(\vec{r}^N\right)}}_{\parbox{1cm}{$Z_\text{conf}\ \text{ ``configurational partition funciton''}$}}}
\end{align}

For simplicity we will focus on these cases, but all methods described here can be easily extended to observables that depend on momenta, too.\\
Could we compute $\int\dd\vec{r}^N e^{-\beta V\left(\vec{r}^N\right)}$?\\
If so, we would know $F = -k_B T \ln(Z)$, i.e. we would have solved the problem completely.\\

(Side remark:\\
We only need $Z_\text{conf}$ because $Z_\text{kin}$ is trivial:
\begin{align}
	Z_\text{kin} &= \int\dd\vec{p}^N\ e^{-\beta\sum\limits_{i=1}^{N}\frac{\left(\vec{p}_i\right)^2}{2m}}\\
	&= \left(\int\dd\vec{p}\ e^{-\beta\frac{\vec{p}^2}{2m}}\right)^N
\end{align}
\begin{align}
	\Rightarrow \sqrt[N]{Z_\text{kin}} &=  \int\dd\vec{p}\ e^{-\beta\frac{\vec{p}^2}{2m}} \Leftrightarrow\sqrt[3N]{Z_\text{kin}} = \int\dd p\ e^{-\beta\frac{p^2}{2m}}
\end{align}

using $\int\dd x\ e^{-ax^2} = \sqrt{\frac{\pi}{a}}\Rightarrow Z_\text{kin} = \left(\frac{2m\pi}{\beta}\right)^N$ Partition function of the ideal gas)\\

Unfortunately, the evaluation of $Z_\text{conf}$ is much harder. However, $\langle A \rangle$ is simpler to compute.\\
We could try to use \textsc{Simpson}'s rule (see \cref{pic:simpsons_rule}) (or a similar approach).\\

\begin{figure}[!h]
\centering
\begin{tikzpicture}
\draw[<->] (0,2) node[left] {f$(x)$} -- (0,0) -- (4,0);
\foreach \x in {0.5, 1,...,3.5} {
\draw (\x, .05) -- (\x, -.05);
}
\foreach \x in {1,2,...,7} {
\node[below] at (\x/2, -0.05) {$x_\x$};
}
\draw (.5,1.8) .. controls (1,0) and (2, 1.7) .. (3.5, 2);
\draw[<->] (0.5, .2) -- node[above] {$h$} (1, .2);
\end{tikzpicture}
\caption{\textsc{Simpson}'s Rule}{$\int\limits_{x_0}^{x_n} f(x)\text{d} x\approx\frac{h}{3}\left[f(x_0) + 4\cdot f(x_1) + 2\cdot f(x_2)+4\cdot f(x_3)+\dots+4\cdot f(x_{n-1}) + f(x_n)\right]$}
\label{pic:simpsons_rule}
\end{figure}

However, $\vec{r}^N$ is of \underline{very high dimension}.\\
If we evaluate $A$ at $m$ points per dimension, we need to compute $m^{3N}$ values. Even for just 100 particles and just 5 points per dimension that is infeasible.\\
$\Rightarrow$ We need a more efficient method.\\

\underline{Random Sampling}\\
We note that:
\begin{align}
	\int\limits_{a}^{b} f(x) \text{d}x &= (b-a)\underbrace{\langle f(x)\rangle}_{\parbox{.5cm}{\hspace*{-1cm}$\text{average of } f \text{ on } [a, b]$}}
\end{align}

$\Rightarrow$ If we randomly (and evenly distributed) draw points $x_i\in[a, b]$ and evaluate $f(x_i)$ we obtain an estimate:
\begin{align}
	\langle f \rangle_\text{est} &= \frac{1}{M} \sum\limits_{i=1}^{M}f(x_i)\\
	I &= \lim\limits_{M\to\infty} (b-a)\frac{1}{M} \sum\limits_{i=1}^{M}f(x_i)
\end{align}

In high dimensions $\langle f \rangle_\text{est}$ converges much faster than any estimate based on sampling a regular grid.\\
The error scales roughly as $\frac{1}{\sqrt{M}}$.
\begin{align}
	\int\limits_{a}^{b} f(x) \text{d}x &\approx (b-a) \langle f\rangle_\text{est} \pm (b-a) \sqrt{\frac{\langle f^2\rangle_\text{est}-\langle f \rangle_\text{est}^2}{M}}\\
	\langle f^2\rangle_\text{est} &= \frac{1}{M} \sum\limits_{i=1}^M f(x_i)^2
\end{align}

This is \underline{not a rigorous statement}. The quality of the fit depends crucially on the details of the function $f$.\\

However, the scaling with $\frac{1}{\sqrt{M}}$ holds independently of the dimension in which we pick the points $x_i$.\\
Thus, unless $f$ is pathological, random sampling performs well in high dimensions.\\

\underline{Exercise:}\\
Compute the area of a circle with diameter 1.\\
\begin{tikzpicture}
		\draw (0,0) rectangle (2,2);
		\draw (1,1) circle (1);
		\foreach \x/\y in {0.2/0.7, 1.3/1.4, 1.9/.4, .7/.1} {
		\filldraw (\x, \y) circle (.03);
	}
	\node[right] at (2.5, 1.5) {Randomly generate points in the square of lengths $1\times1$};
	\node[right] at (2.5, 1) {$\frac{A_\circ}{A_\square}\approx\frac{\text{hits inside }\circ}{\text{tries}}$};
\end{tikzpicture}
Compare this approach to a grid-based method, go to higher dimension and study convergence.

\underline{Importance Sampling}\\
We can improve the quality of the estimate, if we ``weight'' the sampled points, i.e. if we do not draw uniformly.\\
We consider $a=0,\ b=1$ (w.l.o.g.).
\begin{align}
	I &= \int\limits_0^1f(x)\text{d}x = \int\limits_0^1\omega(x)\frac{f(x)}{\omega(x)}\text{d}x \quad\text{for some} \underbrace{\omega(x)>0}_{\parbox{3.7cm}{$\Rightarrow u(x)\ \text{mon. increasing}$}}
\end{align}

We define $\omega(x) := \frac{\text{d}u(x)}{\text{d}x}$, $u(0)=0$, $u(1) = 1$\\
$\Rightarrow \omega(x)$ is normalized on $[0, 1]$
\begin{align}
	\Rightarrow I &= \int\limits_0^1\dd u\ \frac{f(x(u))}{\omega(x(u))}
\end{align}
Now we draw points $u_i$ uniformly (instead of $x_i$)
\begin{align}
	\rightarrow I &\approx \frac{1}{M}\sum\limits_{i=1}^M\frac{f(x(u_i))}{\omega(x(u_i))}
\end{align}
Obviously, if $\omega\sim f$ we would need only one data point to evaluate $I$. (However, to construct $\omega$, we need to make sure that $\int\limits_0^1\omega(x)\text{d}x = 1$ for normalization, i.e. we would need to be able to solve $\int f(x)\text{d}x\dots$)\\
Often we can approximate $f$ by a function $\omega$, which we can normalize analytically. Then importance sampling is very efficient.\\

For the case of thermal averages, \textsc{Metropolis} and \textsc{Teller} suggested a different, but closely related approach: